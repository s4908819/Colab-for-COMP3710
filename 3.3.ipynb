{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOX8lrQ+dAduL9G2KMNY2J2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s4908819/Colab-for-COMP3710/blob/main/3.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看 GPU & 预装的 torch/torchvision 版本（Colab 通常已预装）\n",
        "import torch, platform, os\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "# 打开 TF32、cudnn benchmark（和你 slurm 脚本一致的“加速开关”）\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except:\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQpBSLMcnHvv",
        "outputId": "ecda70c3-c92d-4a32-c366-f7ec66b77eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11\n",
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e039a136-3994-43b4-cd83-92b7bba24a29",
        "id": "50qMGKNlpPJB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fast_cifar10.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile fast_cifar10.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import os, time, argparse\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(planes)\n",
        "        self.down  = None\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        y = self.bn2(self.conv2(y))\n",
        "        if self.down is not None:\n",
        "            x = self.down(x)\n",
        "        return F.relu(x + y, inplace=True)\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_planes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make(64,  2, 1)\n",
        "        self.layer2 = self._make(128, 2, 2)\n",
        "        self.layer3 = self._make(256, 2, 2)\n",
        "        self.layer4 = self._make(512, 2, 2)\n",
        "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc  = nn.Linear(512, num_classes)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def _make(self, planes, blocks, stride):\n",
        "        layers = [BasicBlock(self.in_planes, planes, stride)]\n",
        "        self.in_planes = planes\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.in_planes, planes, 1))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
        "        x = self.avg(x).flatten(1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def tensor_stats(t):\n",
        "    return dict(dtype=str(t.dtype), shape=list(t.shape),\n",
        "                min=float(t.min().item()) if t.numel() else None,\n",
        "                max=float(t.max().item()) if t.numel() else None,\n",
        "                unique=int(t.unique().numel()) if t.numel()<10000 else None)\n",
        "\n",
        "@torch.no_grad()\n",
        "def param_snapshot(model):\n",
        "    picks, snap = [], {}\n",
        "    for n, p in model.named_parameters():\n",
        "        if p.requires_grad and p.ndim >= 2:\n",
        "            picks.append(n)\n",
        "        if len(picks) >= 3:\n",
        "            break\n",
        "    for n in picks:\n",
        "        snap[n] = model.state_dict()[n].float().norm().item()\n",
        "    return picks, snap\n",
        "\n",
        "def param_delta(model, picks, before):\n",
        "    moved = {}\n",
        "    for n in picks:\n",
        "        v = model.state_dict()[n].float().norm().item()\n",
        "        moved[n] = abs(v - before[n])\n",
        "    return moved\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = float(decay)\n",
        "        self.ema = ResNet18(num_classes=model.fc.out_features).to(next(model.parameters()).device)\n",
        "        self.ema.load_state_dict(model.state_dict())\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        msd = model.state_dict(); esd = self.ema.state_dict()\n",
        "        d = self.decay; i = 1.0 - d\n",
        "        for k, v in esd.items():\n",
        "            src = msd[k]\n",
        "            if torch.is_floating_point(v):\n",
        "                v.mul_(d).add_(src, alpha=i)\n",
        "            else:\n",
        "                esd[k].copy_(src)\n",
        "\n",
        "def get_loaders(root, batch_size, num_workers, autoaugment=True):\n",
        "    mean, std = (0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010)\n",
        "    train_tfms = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]\n",
        "    if autoaugment:\n",
        "        train_tfms.append(transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10))\n",
        "    train_tfms += [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
        "    test_tfms  = [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
        "    try:\n",
        "        train = datasets.CIFAR10(root=root, train=True,  download=False, transform=transforms.Compose(train_tfms))\n",
        "        test  = datasets.CIFAR10(root=root, train=False, download=False, transform=transforms.Compose(test_tfms))\n",
        "    except Exception:\n",
        "        train = datasets.CIFAR10(root=root, train=True,  download=True, transform=transforms.Compose(train_tfms))\n",
        "        test  = datasets.CIFAR10(root=root, train=False, download=True, transform=transforms.Compose(test_tfms))\n",
        "    pin = True\n",
        "    persistent = num_workers > 0\n",
        "    prefetch = 4 if num_workers > 0 else None\n",
        "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  num_workers=num_workers,\n",
        "                              pin_memory=pin, persistent_workers=persistent,\n",
        "                              prefetch_factor=prefetch if prefetch else 2)\n",
        "    test_loader  = DataLoader(test,  batch_size=1024,       shuffle=False, num_workers=num_workers,\n",
        "                              pin_memory=pin, persistent_workers=persistent,\n",
        "                              prefetch_factor=prefetch if prefetch else 2)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, amp=True, channels_last=False):\n",
        "    model.eval(); correct=total=0\n",
        "    if amp and device.type == \"cuda\":\n",
        "        ctx = torch.amp.autocast('cuda')\n",
        "    else:\n",
        "        ctx = torch.amp.autocast('cpu')\n",
        "    with ctx:\n",
        "        for x,y in loader:\n",
        "            x = x.to(device, non_blocking=True,\n",
        "                     memory_format=torch.channels_last if channels_last else torch.contiguous_format)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            pred = model(x).argmax(1)\n",
        "            correct += (pred==y).sum().item(); total += y.size(0)\n",
        "    return 100.0*correct/total\n",
        "\n",
        "def debug_one_batch(train_loader, model, device, criterion, optimizer, use_amp=False):\n",
        "    model.train()\n",
        "    xb, yb = next(iter(train_loader))\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    print(\"[DEBUG] y:\", tensor_stats(yb))\n",
        "    if yb.dtype != torch.long:\n",
        "        print(\"!! CrossEntropyLoss 需要 int64 索引标签（非 one-hot）。\")\n",
        "    if yb.ndim != 1:\n",
        "        print(\"!! 目标形状应为 [N]。当前可能是 one-hot 或多维。\")\n",
        "    logits = model(xb)\n",
        "    with torch.no_grad():\n",
        "        probs = torch.softmax(logits, dim=1).mean(0)\n",
        "        print(\"[DEBUG] mean class probs:\", probs.detach().cpu().numpy().round(3).tolist())\n",
        "    loss = criterion(logits, yb)\n",
        "    print(\"[DEBUG] loss(before step):\", float(loss.item()))\n",
        "    picks, snap = param_snapshot(model)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    if use_amp and device.type == \"cuda\":\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            loss2 = criterion(model(xb), yb)\n",
        "        scaler = torch.amp.GradScaler('cuda')\n",
        "        scaler.scale(loss2).backward()\n",
        "        gstats = []\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.grad is not None and p.requires_grad:\n",
        "                gstats.append((n, p.grad.detach().float().norm().item()))\n",
        "                if len(gstats) >= 3: break\n",
        "        print(\"[DEBUG] grad norms:\", [(n, round(v, 6)) for n, v in gstats] or \"all None\")\n",
        "        scaler.step(optimizer); scaler.update()\n",
        "    else:\n",
        "        loss2 = criterion(model(xb), yb)\n",
        "        loss2.backward()\n",
        "        gstats = []\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.grad is not None and p.requires_grad:\n",
        "                gstats.append((n, p.grad.detach().float().norm().item()))\n",
        "                if len(gstats) >= 3: break\n",
        "        print(\"[DEBUG] grad norms:\", [(n, round(v, 6)) for n, v in gstats] or \"all None\")\n",
        "        optimizer.step()\n",
        "    moved = param_delta(model, picks, snap)\n",
        "    print(\"[DEBUG] param moved (|Δ||W||):\", {k: round(v, 6) for k, v in moved.items()})\n",
        "    if all(v == 0.0 for v in moved.values()):\n",
        "        print(\"!! 参数没有变化：可能优化器没 step、梯度为 0/None、或被 AMP/NaN/clip 抑制。\")\n",
        "    with torch.no_grad():\n",
        "        loss_after = criterion(model(xb), yb).item()\n",
        "    print(\"[DEBUG] loss(after step):\", float(loss_after))\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data\", type=str, default=str(Path.home()/ \"datasets/cifar10\"))\n",
        "    ap.add_argument(\"--epochs\", type=int, default=200)\n",
        "    ap.add_argument(\"--batch-size\", type=int, default=1024)\n",
        "    ap.add_argument(\"--lr\", type=float, default=0.4)\n",
        "    ap.add_argument(\"--weight-decay\", type=float, default=5e-4)\n",
        "    ap.add_argument(\"--momentum\", type=float, default=0.9)\n",
        "    ap.add_argument(\"--label-smoothing\", type=float, default=0.1)\n",
        "    ap.add_argument(\"--cosine\", action=\"store_true\")\n",
        "    ap.add_argument(\"--amp\", action=\"store_true\")\n",
        "    ap.add_argument(\"--channels-last\", action=\"store_true\")\n",
        "    ap.add_argument(\"--ema\", type=float, default=0.999)\n",
        "    ap.add_argument(\"--compile\", action=\"store_true\")\n",
        "    ap.add_argument(\"--target-acc\", type=float, default=93.0)\n",
        "    ap.add_argument(\"--evaluate\", action=\"store_true\")\n",
        "    ap.add_argument(\"--debug-one-batch\", action=\"store_true\")\n",
        "    ap.add_argument(\"--tiny-overfit\", action=\"store_true\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Colab 上 num_workers 取 2~4 更稳（persistent_workers=True 需要 >0）\n",
        "    num_workers = 2\n",
        "    train_loader, test_loader = get_loaders(args.data, args.batch_size, num_workers, autoaugment=True)\n",
        "\n",
        "    model = ResNet18().to(device)\n",
        "    if args.channels_last:\n",
        "        model = model.to(memory_format=torch.channels_last)\n",
        "    if args.compile:\n",
        "        try:\n",
        "            model = torch.compile(model, mode=\"max-autotune\")\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] torch.compile failed: {e}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing).to(device)\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=args.epochs) if args.cosine \\\n",
        "          else torch.optim.lr_scheduler.MultiStepLR(opt, [100,150], 0.1)\n",
        "    ema = EMA(model, decay=args.ema) if args.ema>0 else None\n",
        "\n",
        "    if args.tiny_overfit:\n",
        "        subset_n = 512\n",
        "        from torch.utils.data import Subset\n",
        "        train_idx = list(range(min(subset_n, len(train_loader.dataset))))\n",
        "        train_loader = DataLoader(Subset(train_loader.dataset, train_idx),\n",
        "                                  batch_size=128, shuffle=True, num_workers=2,\n",
        "                                  pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
        "        base_lr = 0.4 * (128 / 1024)\n",
        "        for g in opt.param_groups:\n",
        "            g['lr'] = base_lr\n",
        "            g['weight_decay'] = 0.0\n",
        "        print(f\"[TinyOverfit] subset={len(train_idx)}, batch=128, lr={base_lr:.4f}, wd=0.0\")\n",
        "\n",
        "    if args.debug_one_batch:\n",
        "        debug_one_batch(train_loader, model, device, criterion, opt, use_amp=args.amp and device.type==\"cuda\")\n",
        "        return\n",
        "\n",
        "    t0 = time.monotonic()\n",
        "    base = evaluate(model, test_loader, device, amp=args.amp, channels_last=args.channels_last)\n",
        "    print(f\"[Eval@Start] Acc={base:.2f}%  Device={device}  CUDA={torch.version.cuda}\")\n",
        "\n",
        "    if args.evaluate:\n",
        "        print(f\"[Evaluate only] Elapsed {time.monotonic()-t0:.2f}s\"); return\n",
        "\n",
        "    scaler = torch.amp.GradScaler('cuda') if (args.amp and device.type==\"cuda\") else None\n",
        "    best = base\n",
        "    for ep in range(args.epochs):\n",
        "        model.train(); loss_sum=0.0\n",
        "        for x,y in train_loader:\n",
        "            x = x.to(device, non_blocking=True,\n",
        "                     memory_format=torch.channels_last if args.channels_last else torch.contiguous_format)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            if scaler is not None:\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    logits = model(x); loss = criterion(logits, y)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt); scaler.update()\n",
        "            else:\n",
        "                logits = model(x); loss = criterion(logits, y)\n",
        "                loss.backward(); opt.step()\n",
        "            if ema: ema.update(model)\n",
        "            loss_sum += loss.item()\n",
        "        sch.step()\n",
        "        eval_model = ema.ema if ema else model\n",
        "        acc = evaluate(eval_model, test_loader, device, amp=bool(scaler), channels_last=args.channels_last)\n",
        "        best = max(best, acc); elapsed = time.monotonic()-t0\n",
        "        print(f\"[Epoch {ep+1}/{args.epochs}] loss={loss_sum/len(train_loader):.4f}  acc={acc:.2f}%  best={best:.2f}%  time={elapsed:.1f}s\")\n",
        "        if acc >= args.target_acc:\n",
        "            print(f\"[Reached {args.target_acc:.1f}%] Time-to-accuracy: {elapsed:.2f} seconds\")\n",
        "            break\n",
        "    eval_model = ema.ema if ema else model\n",
        "    final = evaluate(eval_model, test_loader, device, amp=bool(scaler), channels_last=args.channels_last)\n",
        "    print(f\"[Final] Acc={final:.2f}%  Total Time={time.monotonic()-t0:.2f}s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Fast CIFAR-10 训练（无 label smoothing / 无 EMA，自动 batch，达成 93% 自动停） ===\n",
        "from pathlib import Path\n",
        "import torch, os, platform\n",
        "\n",
        "# 1) 环境 & 数据目录准备（Colab 默认在 /content）\n",
        "data_root = \"/content\"\n",
        "\n",
        "# 兼容你之前手动放的数据：把 cifar-10-python 改成 torchvision 期望的名字\n",
        "if os.path.isdir(f\"{data_root}/cifar-10-python\") and not os.path.isdir(f\"{data_root}/cifar-10-batches-py\"):\n",
        "    os.rename(f\"{data_root}/cifar-10-python\", f\"{data_root}/cifar-10-batches-py\")\n",
        "\n",
        "# 2) GPU 与加速开关\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "\n",
        "# 3) 自动选择 batch size 与学习率（更稳的组合）\n",
        "if torch.cuda.is_available():\n",
        "    name = torch.cuda.get_device_name(0).lower()\n",
        "    if \"t4\" in name or \"p100\" in name:   # 显存较小\n",
        "        batch = 512;  lr = 0.15\n",
        "    elif \"v100\" in name or \"l4\" in name or \"a10\" in name:\n",
        "        batch = 768;  lr = 0.20\n",
        "    else:                                 # A100/更强\n",
        "        batch = 1024; lr = 0.20\n",
        "else:\n",
        "    batch = 128;  lr = 0.10\n",
        "\n",
        "print(f\"Chosen batch size: {batch}, lr: {lr}\")\n",
        "\n",
        "# 4) 开始训练：无 label smoothing / 无 EMA，AMP + channels_last + cosine 调度\n",
        "#    提前停止阈值 target-acc=93.0\n",
        "data_root_var = data_root  # 供 shell 展开\n",
        "batch_var = batch\n",
        "lr_var = lr\n",
        "\n",
        "# 用 IPython 变量替换（$var 语法）\n",
        "!python fast_cifar10.py \\\n",
        "  --data \"$data_root_var\" \\\n",
        "  --epochs 400 \\\n",
        "  --target-acc 93.0 \\\n",
        "  --batch-size $batch_var \\\n",
        "  --lr $lr_var \\\n",
        "  --weight-decay 5e-4 \\\n",
        "  --label-smoothing 0.0 \\\n",
        "  --ema 0.0 \\\n",
        "  --cosine \\\n",
        "  --amp \\\n",
        "  --channels-last\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bwfha7onO0r",
        "outputId": "70244e7a-b05b-4bb6-f2d5-5500b5c45bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11\n",
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n",
            "Chosen batch size: 768, lr: 0.2\n",
            "100% 170M/170M [00:10<00:00, 16.3MB/s]\n",
            "[Eval@Start] Acc=10.00%  Device=cuda  CUDA=12.6\n",
            "[Epoch 1/400] loss=3.1838  acc=12.94%  best=12.94%  time=55.9s\n",
            "[Epoch 2/400] loss=2.2582  acc=17.50%  best=17.50%  time=75.7s\n",
            "[Epoch 3/400] loss=2.1624  acc=23.60%  best=23.60%  time=95.5s\n",
            "[Epoch 4/400] loss=2.0581  acc=25.33%  best=25.33%  time=115.4s\n",
            "[Epoch 5/400] loss=1.9460  acc=32.66%  best=32.66%  time=135.5s\n",
            "[Epoch 6/400] loss=1.8478  acc=34.18%  best=34.18%  time=155.5s\n",
            "[Epoch 7/400] loss=1.7380  acc=35.39%  best=35.39%  time=175.4s\n",
            "[Epoch 8/400] loss=1.5996  acc=42.75%  best=42.75%  time=195.2s\n",
            "[Epoch 9/400] loss=1.4765  acc=43.85%  best=43.85%  time=215.2s\n",
            "[Epoch 10/400] loss=1.3422  acc=54.86%  best=54.86%  time=235.2s\n",
            "[Epoch 11/400] loss=1.2194  acc=59.29%  best=59.29%  time=255.5s\n",
            "[Epoch 12/400] loss=1.1050  acc=60.55%  best=60.55%  time=275.4s\n",
            "[Epoch 13/400] loss=1.0127  acc=65.51%  best=65.51%  time=295.4s\n",
            "[Epoch 14/400] loss=0.9517  acc=69.39%  best=69.39%  time=315.4s\n",
            "[Epoch 15/400] loss=0.8771  acc=73.74%  best=73.74%  time=335.3s\n",
            "[Epoch 16/400] loss=0.8170  acc=79.06%  best=79.06%  time=355.2s\n",
            "[Epoch 17/400] loss=0.7797  acc=74.03%  best=79.06%  time=375.2s\n",
            "[Epoch 18/400] loss=0.7208  acc=75.03%  best=79.06%  time=395.1s\n",
            "[Epoch 19/400] loss=0.7091  acc=78.68%  best=79.06%  time=414.9s\n",
            "[Epoch 20/400] loss=0.6705  acc=80.74%  best=80.74%  time=434.9s\n",
            "[Epoch 21/400] loss=0.6356  acc=72.46%  best=80.74%  time=454.8s\n",
            "[Epoch 22/400] loss=0.6301  acc=79.55%  best=80.74%  time=474.7s\n",
            "[Epoch 23/400] loss=0.5905  acc=77.58%  best=80.74%  time=494.6s\n",
            "[Epoch 24/400] loss=0.5865  acc=75.88%  best=80.74%  time=514.5s\n",
            "[Epoch 25/400] loss=0.5876  acc=78.56%  best=80.74%  time=534.4s\n",
            "[Epoch 26/400] loss=0.5859  acc=76.37%  best=80.74%  time=554.4s\n",
            "[Epoch 27/400] loss=0.5462  acc=83.65%  best=83.65%  time=574.2s\n",
            "[Epoch 28/400] loss=0.5269  acc=78.92%  best=83.65%  time=594.2s\n",
            "[Epoch 29/400] loss=0.5362  acc=75.49%  best=83.65%  time=614.3s\n",
            "[Epoch 30/400] loss=0.5058  acc=77.60%  best=83.65%  time=634.4s\n",
            "[Epoch 31/400] loss=0.5153  acc=82.37%  best=83.65%  time=654.3s\n",
            "[Epoch 32/400] loss=0.4973  acc=81.40%  best=83.65%  time=674.3s\n",
            "[Epoch 33/400] loss=0.4898  acc=83.41%  best=83.65%  time=694.3s\n",
            "[Epoch 34/400] loss=0.4840  acc=80.10%  best=83.65%  time=714.4s\n",
            "[Epoch 35/400] loss=0.4792  acc=85.77%  best=85.77%  time=734.4s\n",
            "[Epoch 36/400] loss=0.4735  acc=80.30%  best=85.77%  time=754.2s\n",
            "[Epoch 37/400] loss=0.4779  acc=79.97%  best=85.77%  time=774.2s\n",
            "[Epoch 38/400] loss=0.4763  acc=84.14%  best=85.77%  time=794.4s\n",
            "[Epoch 39/400] loss=0.4598  acc=83.54%  best=85.77%  time=814.8s\n",
            "[Epoch 40/400] loss=0.4675  acc=82.19%  best=85.77%  time=835.1s\n",
            "[Epoch 41/400] loss=0.4531  acc=81.34%  best=85.77%  time=855.2s\n",
            "[Epoch 42/400] loss=0.4489  acc=82.87%  best=85.77%  time=875.3s\n",
            "[Epoch 43/400] loss=0.4339  acc=84.19%  best=85.77%  time=895.3s\n",
            "[Epoch 44/400] loss=0.4273  acc=81.55%  best=85.77%  time=915.4s\n",
            "[Epoch 45/400] loss=0.4294  acc=81.64%  best=85.77%  time=935.3s\n",
            "[Epoch 46/400] loss=0.4335  acc=85.46%  best=85.77%  time=955.4s\n",
            "[Epoch 47/400] loss=0.4332  acc=85.44%  best=85.77%  time=975.5s\n",
            "[Epoch 48/400] loss=0.4196  acc=84.25%  best=85.77%  time=995.5s\n",
            "[Epoch 49/400] loss=0.4215  acc=84.10%  best=85.77%  time=1015.6s\n",
            "[Epoch 50/400] loss=0.4236  acc=83.92%  best=85.77%  time=1035.7s\n",
            "[Epoch 51/400] loss=0.4076  acc=83.77%  best=85.77%  time=1056.3s\n",
            "[Epoch 52/400] loss=0.4137  acc=85.73%  best=85.77%  time=1076.7s\n",
            "[Epoch 53/400] loss=0.4144  acc=84.51%  best=85.77%  time=1097.3s\n",
            "[Epoch 54/400] loss=0.4157  acc=85.87%  best=85.87%  time=1117.7s\n",
            "[Epoch 55/400] loss=0.4029  acc=84.15%  best=85.87%  time=1138.0s\n",
            "[Epoch 56/400] loss=0.3995  acc=83.25%  best=85.87%  time=1158.4s\n",
            "[Epoch 57/400] loss=0.3994  acc=79.73%  best=85.87%  time=1178.9s\n",
            "[Epoch 58/400] loss=0.4051  acc=87.13%  best=87.13%  time=1199.0s\n",
            "[Epoch 59/400] loss=0.3994  acc=83.94%  best=87.13%  time=1219.3s\n",
            "[Epoch 60/400] loss=0.4015  acc=85.73%  best=87.13%  time=1239.6s\n",
            "[Epoch 61/400] loss=0.3828  acc=84.53%  best=87.13%  time=1259.5s\n",
            "[Epoch 62/400] loss=0.3756  acc=84.68%  best=87.13%  time=1279.7s\n",
            "[Epoch 63/400] loss=0.3763  acc=88.24%  best=88.24%  time=1299.7s\n",
            "[Epoch 64/400] loss=0.3783  acc=86.72%  best=88.24%  time=1319.8s\n",
            "[Epoch 65/400] loss=0.3930  acc=84.03%  best=88.24%  time=1339.8s\n",
            "[Epoch 66/400] loss=0.3930  acc=87.78%  best=88.24%  time=1360.1s\n",
            "[Epoch 67/400] loss=0.3787  acc=88.38%  best=88.38%  time=1380.2s\n",
            "[Epoch 68/400] loss=0.3688  acc=85.48%  best=88.38%  time=1400.2s\n",
            "[Epoch 69/400] loss=0.3737  acc=83.01%  best=88.38%  time=1420.1s\n",
            "[Epoch 70/400] loss=0.3647  acc=84.94%  best=88.38%  time=1440.1s\n",
            "[Epoch 71/400] loss=0.3764  acc=82.05%  best=88.38%  time=1459.9s\n",
            "[Epoch 72/400] loss=0.3824  acc=85.10%  best=88.38%  time=1479.8s\n",
            "[Epoch 73/400] loss=0.3644  acc=85.78%  best=88.38%  time=1499.8s\n",
            "[Epoch 74/400] loss=0.3614  acc=81.55%  best=88.38%  time=1519.8s\n",
            "[Epoch 75/400] loss=0.3783  acc=87.83%  best=88.38%  time=1539.9s\n",
            "[Epoch 76/400] loss=0.3688  acc=84.32%  best=88.38%  time=1559.9s\n",
            "[Epoch 77/400] loss=0.3850  acc=86.61%  best=88.38%  time=1579.8s\n",
            "[Epoch 78/400] loss=0.3710  acc=88.62%  best=88.62%  time=1599.7s\n",
            "[Epoch 79/400] loss=0.3634  acc=85.73%  best=88.62%  time=1619.6s\n",
            "[Epoch 80/400] loss=0.3570  acc=85.46%  best=88.62%  time=1639.5s\n",
            "[Epoch 81/400] loss=0.3770  acc=84.46%  best=88.62%  time=1659.6s\n",
            "[Epoch 82/400] loss=0.3580  acc=85.73%  best=88.62%  time=1679.5s\n",
            "[Epoch 83/400] loss=0.3456  acc=85.36%  best=88.62%  time=1699.4s\n",
            "[Epoch 84/400] loss=0.3716  acc=86.95%  best=88.62%  time=1719.4s\n",
            "[Epoch 85/400] loss=0.3453  acc=88.45%  best=88.62%  time=1739.3s\n",
            "[Epoch 86/400] loss=0.3372  acc=85.63%  best=88.62%  time=1759.2s\n",
            "[Epoch 87/400] loss=0.3624  acc=82.46%  best=88.62%  time=1779.1s\n",
            "[Epoch 88/400] loss=0.3577  acc=83.71%  best=88.62%  time=1799.0s\n",
            "[Epoch 89/400] loss=0.3486  acc=87.44%  best=88.62%  time=1818.9s\n",
            "[Epoch 90/400] loss=0.3600  acc=87.89%  best=88.62%  time=1838.8s\n",
            "[Epoch 91/400] loss=0.3398  acc=85.84%  best=88.62%  time=1858.8s\n",
            "[Epoch 92/400] loss=0.3337  acc=85.78%  best=88.62%  time=1878.7s\n",
            "[Epoch 93/400] loss=0.3473  acc=88.65%  best=88.65%  time=1898.7s\n",
            "[Epoch 94/400] loss=0.3319  acc=89.03%  best=89.03%  time=1918.7s\n",
            "[Epoch 95/400] loss=0.3364  acc=85.13%  best=89.03%  time=1938.8s\n",
            "[Epoch 96/400] loss=0.3463  acc=86.58%  best=89.03%  time=1958.9s\n",
            "[Epoch 97/400] loss=0.3456  acc=85.60%  best=89.03%  time=1978.9s\n",
            "[Epoch 98/400] loss=0.3448  acc=88.80%  best=89.03%  time=1998.9s\n",
            "[Epoch 99/400] loss=0.3422  acc=85.61%  best=89.03%  time=2018.7s\n",
            "[Epoch 100/400] loss=0.3333  acc=86.18%  best=89.03%  time=2038.7s\n",
            "[Epoch 101/400] loss=0.3283  acc=88.35%  best=89.03%  time=2058.8s\n",
            "[Epoch 102/400] loss=0.3317  acc=88.75%  best=89.03%  time=2078.7s\n",
            "[Epoch 103/400] loss=0.3472  acc=86.26%  best=89.03%  time=2098.6s\n",
            "[Epoch 104/400] loss=0.3441  acc=87.76%  best=89.03%  time=2118.6s\n",
            "[Epoch 105/400] loss=0.3419  acc=84.25%  best=89.03%  time=2138.7s\n",
            "[Epoch 106/400] loss=0.3397  acc=87.80%  best=89.03%  time=2158.7s\n",
            "[Epoch 107/400] loss=0.3439  acc=87.38%  best=89.03%  time=2178.6s\n",
            "[Epoch 108/400] loss=0.3424  acc=85.65%  best=89.03%  time=2198.6s\n",
            "[Epoch 109/400] loss=0.3247  acc=88.17%  best=89.03%  time=2218.7s\n",
            "[Epoch 110/400] loss=0.3278  acc=87.52%  best=89.03%  time=2238.7s\n",
            "[Epoch 111/400] loss=0.3149  acc=89.73%  best=89.73%  time=2258.7s\n",
            "[Epoch 112/400] loss=0.3190  acc=89.45%  best=89.73%  time=2278.7s\n",
            "[Epoch 113/400] loss=0.3179  acc=86.82%  best=89.73%  time=2298.6s\n",
            "[Epoch 114/400] loss=0.3439  acc=83.46%  best=89.73%  time=2318.6s\n",
            "[Epoch 115/400] loss=0.3153  acc=86.34%  best=89.73%  time=2338.4s\n",
            "[Epoch 116/400] loss=0.3404  acc=85.86%  best=89.73%  time=2358.5s\n",
            "[Epoch 117/400] loss=0.3253  acc=85.36%  best=89.73%  time=2378.4s\n",
            "[Epoch 118/400] loss=0.3152  acc=88.50%  best=89.73%  time=2398.6s\n",
            "[Epoch 119/400] loss=0.3097  acc=86.44%  best=89.73%  time=2418.5s\n",
            "[Epoch 120/400] loss=0.3122  acc=87.39%  best=89.73%  time=2438.5s\n",
            "[Epoch 121/400] loss=0.3205  acc=85.65%  best=89.73%  time=2458.8s\n",
            "[Epoch 122/400] loss=0.3318  acc=87.91%  best=89.73%  time=2479.2s\n",
            "[Epoch 123/400] loss=0.3395  acc=89.77%  best=89.77%  time=2499.3s\n",
            "[Epoch 124/400] loss=0.3155  acc=86.94%  best=89.77%  time=2519.7s\n",
            "[Epoch 125/400] loss=0.3274  acc=88.37%  best=89.77%  time=2540.1s\n",
            "[Epoch 126/400] loss=0.3093  acc=90.11%  best=90.11%  time=2560.5s\n",
            "[Epoch 127/400] loss=0.2939  acc=88.24%  best=90.11%  time=2581.0s\n",
            "[Epoch 128/400] loss=0.3104  acc=84.47%  best=90.11%  time=2601.5s\n",
            "[Epoch 129/400] loss=0.3000  acc=89.68%  best=90.11%  time=2621.9s\n",
            "[Epoch 130/400] loss=0.2981  acc=86.60%  best=90.11%  time=2642.3s\n",
            "[Epoch 131/400] loss=0.3086  acc=88.41%  best=90.11%  time=2662.6s\n",
            "[Epoch 132/400] loss=0.2847  acc=87.52%  best=90.11%  time=2682.9s\n",
            "[Epoch 133/400] loss=0.3085  acc=88.41%  best=90.11%  time=2703.3s\n",
            "[Epoch 134/400] loss=0.3020  acc=78.37%  best=90.11%  time=2723.7s\n",
            "[Epoch 135/400] loss=0.3251  acc=87.38%  best=90.11%  time=2743.8s\n",
            "[Epoch 136/400] loss=0.2888  acc=89.76%  best=90.11%  time=2764.1s\n",
            "[Epoch 137/400] loss=0.3161  acc=87.68%  best=90.11%  time=2784.5s\n",
            "[Epoch 138/400] loss=0.3124  acc=88.75%  best=90.11%  time=2804.5s\n",
            "[Epoch 139/400] loss=0.3001  acc=89.82%  best=90.11%  time=2824.4s\n",
            "[Epoch 140/400] loss=0.2842  acc=89.82%  best=90.11%  time=2844.4s\n",
            "[Epoch 141/400] loss=0.2953  acc=88.89%  best=90.11%  time=2864.2s\n",
            "[Epoch 142/400] loss=0.2938  acc=88.20%  best=90.11%  time=2884.1s\n",
            "[Epoch 143/400] loss=0.2836  acc=89.47%  best=90.11%  time=2904.0s\n",
            "[Epoch 144/400] loss=0.2852  acc=89.10%  best=90.11%  time=2924.0s\n",
            "[Epoch 145/400] loss=0.3049  acc=90.13%  best=90.13%  time=2944.0s\n",
            "[Epoch 146/400] loss=0.2911  acc=88.76%  best=90.13%  time=2964.1s\n",
            "[Epoch 147/400] loss=0.2842  acc=90.27%  best=90.27%  time=2983.9s\n",
            "[Epoch 148/400] loss=0.2756  acc=88.77%  best=90.27%  time=3003.8s\n",
            "[Epoch 149/400] loss=0.2835  acc=86.14%  best=90.27%  time=3023.9s\n",
            "[Epoch 150/400] loss=0.2898  acc=89.58%  best=90.27%  time=3043.8s\n",
            "[Epoch 151/400] loss=0.2821  acc=89.94%  best=90.27%  time=3063.8s\n",
            "[Epoch 152/400] loss=0.3092  acc=89.20%  best=90.27%  time=3083.8s\n",
            "[Epoch 153/400] loss=0.2835  acc=86.86%  best=90.27%  time=3103.8s\n",
            "[Epoch 154/400] loss=0.2861  acc=89.09%  best=90.27%  time=3123.7s\n",
            "[Epoch 155/400] loss=0.2812  acc=89.86%  best=90.27%  time=3143.6s\n",
            "[Epoch 156/400] loss=0.2784  acc=91.43%  best=91.43%  time=3163.6s\n",
            "[Epoch 157/400] loss=0.2773  acc=87.21%  best=91.43%  time=3183.6s\n",
            "[Epoch 158/400] loss=0.2775  acc=88.49%  best=91.43%  time=3203.5s\n",
            "[Epoch 159/400] loss=0.2995  acc=89.73%  best=91.43%  time=3223.5s\n",
            "[Epoch 160/400] loss=0.2824  acc=87.50%  best=91.43%  time=3243.5s\n",
            "[Epoch 161/400] loss=0.2853  acc=87.20%  best=91.43%  time=3263.3s\n",
            "[Epoch 162/400] loss=0.2673  acc=90.57%  best=91.43%  time=3283.4s\n",
            "[Epoch 163/400] loss=0.2640  acc=90.29%  best=91.43%  time=3303.3s\n",
            "[Epoch 164/400] loss=0.2676  acc=90.96%  best=91.43%  time=3323.2s\n",
            "[Epoch 165/400] loss=0.2621  acc=89.49%  best=91.43%  time=3343.2s\n",
            "[Epoch 166/400] loss=0.2663  acc=88.90%  best=91.43%  time=3363.2s\n",
            "[Epoch 167/400] loss=0.2667  acc=87.27%  best=91.43%  time=3383.1s\n",
            "[Epoch 168/400] loss=0.2619  acc=90.13%  best=91.43%  time=3403.0s\n",
            "[Epoch 169/400] loss=0.2607  acc=88.07%  best=91.43%  time=3422.9s\n",
            "[Epoch 170/400] loss=0.2765  acc=91.01%  best=91.43%  time=3442.9s\n",
            "[Epoch 171/400] loss=0.2620  acc=90.92%  best=91.43%  time=3462.9s\n",
            "[Epoch 172/400] loss=0.2522  acc=90.69%  best=91.43%  time=3482.9s\n",
            "[Epoch 173/400] loss=0.2531  acc=87.82%  best=91.43%  time=3502.9s\n",
            "[Epoch 174/400] loss=0.2679  acc=89.78%  best=91.43%  time=3523.1s\n",
            "[Epoch 175/400] loss=0.2505  acc=89.73%  best=91.43%  time=3543.2s\n",
            "[Epoch 176/400] loss=0.2474  acc=89.15%  best=91.43%  time=3563.6s\n",
            "[Epoch 177/400] loss=0.2506  acc=91.41%  best=91.43%  time=3583.7s\n",
            "[Epoch 178/400] loss=0.2561  acc=90.05%  best=91.43%  time=3603.7s\n",
            "[Epoch 179/400] loss=0.2501  acc=89.69%  best=91.43%  time=3623.8s\n",
            "[Epoch 180/400] loss=0.2672  acc=90.71%  best=91.43%  time=3643.7s\n",
            "[Epoch 181/400] loss=0.2788  acc=91.20%  best=91.43%  time=3663.8s\n",
            "[Epoch 182/400] loss=0.2640  acc=90.29%  best=91.43%  time=3683.7s\n",
            "[Epoch 183/400] loss=0.2508  acc=91.09%  best=91.43%  time=3703.7s\n",
            "[Epoch 184/400] loss=0.2390  acc=89.96%  best=91.43%  time=3723.7s\n",
            "[Epoch 185/400] loss=0.2484  acc=91.60%  best=91.60%  time=3743.6s\n",
            "[Epoch 186/400] loss=0.2216  acc=90.43%  best=91.60%  time=3763.5s\n",
            "[Epoch 187/400] loss=0.2334  acc=91.31%  best=91.60%  time=3783.4s\n",
            "[Epoch 188/400] loss=0.2589  acc=88.59%  best=91.60%  time=3803.4s\n",
            "[Epoch 189/400] loss=0.2305  acc=90.53%  best=91.60%  time=3823.5s\n",
            "[Epoch 190/400] loss=0.2410  acc=90.95%  best=91.60%  time=3843.4s\n",
            "[Epoch 191/400] loss=0.2432  acc=90.58%  best=91.60%  time=3863.4s\n",
            "[Epoch 192/400] loss=0.2421  acc=90.21%  best=91.60%  time=3883.6s\n",
            "[Epoch 193/400] loss=0.2351  acc=90.06%  best=91.60%  time=3903.6s\n",
            "[Epoch 194/400] loss=0.2293  acc=89.04%  best=91.60%  time=3923.5s\n",
            "[Epoch 195/400] loss=0.2664  acc=90.00%  best=91.60%  time=3943.4s\n",
            "[Epoch 196/400] loss=0.2432  acc=88.75%  best=91.60%  time=3963.3s\n",
            "[Epoch 197/400] loss=0.2348  acc=90.43%  best=91.60%  time=3983.5s\n",
            "[Epoch 198/400] loss=0.2318  acc=91.52%  best=91.60%  time=4003.6s\n",
            "[Epoch 199/400] loss=0.2285  acc=89.74%  best=91.60%  time=4023.5s\n",
            "[Epoch 200/400] loss=0.2472  acc=91.21%  best=91.60%  time=4043.4s\n",
            "[Epoch 201/400] loss=0.2238  acc=90.20%  best=91.60%  time=4063.3s\n",
            "[Epoch 202/400] loss=0.2244  acc=90.45%  best=91.60%  time=4083.4s\n",
            "[Epoch 203/400] loss=0.2248  acc=91.44%  best=91.60%  time=4103.2s\n",
            "[Epoch 204/400] loss=0.2217  acc=90.58%  best=91.60%  time=4123.1s\n",
            "[Epoch 205/400] loss=0.2366  acc=91.03%  best=91.60%  time=4143.0s\n",
            "[Epoch 206/400] loss=0.2321  acc=91.41%  best=91.60%  time=4163.0s\n",
            "[Epoch 207/400] loss=0.2220  acc=92.05%  best=92.05%  time=4183.0s\n",
            "[Epoch 208/400] loss=0.2140  acc=91.04%  best=92.05%  time=4202.9s\n",
            "[Epoch 209/400] loss=0.2130  acc=89.13%  best=92.05%  time=4222.8s\n",
            "[Epoch 210/400] loss=0.2234  acc=92.23%  best=92.23%  time=4242.6s\n",
            "[Epoch 211/400] loss=0.2201  acc=91.31%  best=92.23%  time=4262.6s\n",
            "[Epoch 212/400] loss=0.2243  acc=91.31%  best=92.23%  time=4282.5s\n",
            "[Epoch 213/400] loss=0.2242  acc=90.92%  best=92.23%  time=4302.4s\n",
            "[Epoch 214/400] loss=0.2050  acc=92.26%  best=92.26%  time=4322.2s\n",
            "[Epoch 215/400] loss=0.2016  acc=91.77%  best=92.26%  time=4342.1s\n",
            "[Epoch 216/400] loss=0.2235  acc=91.34%  best=92.26%  time=4362.1s\n",
            "[Epoch 217/400] loss=0.2306  acc=92.30%  best=92.30%  time=4382.0s\n",
            "[Epoch 218/400] loss=0.2003  acc=91.48%  best=92.30%  time=4402.0s\n",
            "[Epoch 219/400] loss=0.2007  acc=91.12%  best=92.30%  time=4421.9s\n",
            "[Epoch 220/400] loss=0.1969  acc=91.53%  best=92.30%  time=4441.9s\n",
            "[Epoch 221/400] loss=0.2040  acc=92.52%  best=92.52%  time=4462.0s\n",
            "[Epoch 222/400] loss=0.1920  acc=91.14%  best=92.52%  time=4481.9s\n",
            "[Epoch 223/400] loss=0.2115  acc=88.27%  best=92.52%  time=4502.1s\n",
            "[Epoch 224/400] loss=0.1983  acc=92.17%  best=92.52%  time=4522.0s\n",
            "[Epoch 225/400] loss=0.1956  acc=92.10%  best=92.52%  time=4542.0s\n",
            "[Epoch 226/400] loss=0.2061  acc=91.46%  best=92.52%  time=4562.1s\n",
            "[Epoch 227/400] loss=0.2169  acc=91.95%  best=92.52%  time=4582.3s\n",
            "[Epoch 228/400] loss=0.2123  acc=91.37%  best=92.52%  time=4602.2s\n",
            "[Epoch 229/400] loss=0.2093  acc=91.36%  best=92.52%  time=4622.7s\n",
            "[Epoch 230/400] loss=0.2098  acc=92.41%  best=92.52%  time=4643.2s\n",
            "[Epoch 231/400] loss=0.2064  acc=91.87%  best=92.52%  time=4663.6s\n",
            "[Epoch 232/400] loss=0.1952  acc=91.89%  best=92.52%  time=4683.8s\n",
            "[Epoch 233/400] loss=0.1885  acc=92.02%  best=92.52%  time=4704.2s\n",
            "[Epoch 234/400] loss=0.1876  acc=91.61%  best=92.52%  time=4724.5s\n",
            "[Epoch 235/400] loss=0.1970  acc=90.87%  best=92.52%  time=4744.9s\n",
            "[Epoch 236/400] loss=0.1994  acc=91.90%  best=92.52%  time=4765.1s\n",
            "[Epoch 237/400] loss=0.2004  acc=92.02%  best=92.52%  time=4785.3s\n",
            "[Epoch 238/400] loss=0.1811  acc=91.76%  best=92.52%  time=4805.7s\n",
            "[Epoch 239/400] loss=0.1884  acc=92.48%  best=92.52%  time=4825.7s\n",
            "[Epoch 240/400] loss=0.1904  acc=92.71%  best=92.71%  time=4845.6s\n",
            "[Epoch 241/400] loss=0.1777  acc=91.08%  best=92.71%  time=4865.8s\n",
            "[Epoch 242/400] loss=0.1888  acc=92.06%  best=92.71%  time=4885.8s\n",
            "[Epoch 243/400] loss=0.1862  acc=91.13%  best=92.71%  time=4905.7s\n",
            "[Epoch 244/400] loss=0.1976  acc=92.26%  best=92.71%  time=4925.6s\n",
            "[Epoch 245/400] loss=0.1863  acc=91.78%  best=92.71%  time=4945.5s\n",
            "[Epoch 246/400] loss=0.1859  acc=92.39%  best=92.71%  time=4965.5s\n",
            "[Epoch 247/400] loss=0.1683  acc=92.26%  best=92.71%  time=4985.5s\n",
            "[Epoch 248/400] loss=0.1870  acc=92.54%  best=92.71%  time=5005.7s\n",
            "[Epoch 249/400] loss=0.1769  acc=91.62%  best=92.71%  time=5025.7s\n",
            "[Epoch 250/400] loss=0.1725  acc=93.03%  best=93.03%  time=5045.8s\n",
            "[Reached 93.0%] Time-to-accuracy: 5045.75 seconds\n",
            "[Final] Acc=93.03%  Total Time=5046.90s\n"
          ]
        }
      ]
    }
  ]
}