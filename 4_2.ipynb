{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNMuc13p5vSbbWSR0PTo9at",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s4908819/Colab-for-COMP3710/blob/main/4_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eynBQQSoqiyr",
        "outputId": "f406f578-b6c0-4410-e826-fdd2facf1d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 18 09:34:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Python 3.12.11\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!python -V\n",
        "# 常用包（nibabel 读 NIfTI；tqdm 进度条；umap-learn 可视化可选；opencv-python 可选）\n",
        "!pip -q install nibabel tqdm umap-learn opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir -p datasets scripts data results\n",
        "!touch datasets/__init__.py scripts/__init__.py\n",
        "!unzip -q /content/keras_png_slices_data.zip -d /content/data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac48tf1oNMOt",
        "outputId": "18ccd156-0444-49de-b7b2-643fc5696331"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile datasets/oasis2d.py\n",
        "# datasets/oasis2d.py\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np, torch\n",
        "import nibabel as nib\n",
        "from skimage.transform import resize as sk_resize\n",
        "# --- 新增：PNG/JPG 读取支持 ---\n",
        "from PIL import Image\n",
        "\n",
        "def _load_array(path: str) -> np.ndarray:\n",
        "    p = path.lower()\n",
        "    if p.endswith(\".npy\"):\n",
        "        return np.load(path)\n",
        "    if p.endswith(\".npz\"):\n",
        "        data = np.load(path); key = list(data.keys())[0]\n",
        "        return data[key]\n",
        "    if p.endswith(\".nii\") or p.endswith(\".nii.gz\"):\n",
        "        return np.asarray(nib.load(path).get_fdata())\n",
        "    # --- 新增：PNG/JPG（灰度） ---\n",
        "    if p.endswith(\".png\") or p.endswith(\".jpg\") or p.endswith(\".jpeg\"):\n",
        "        img = Image.open(path)\n",
        "        if img.mode != \"L\":  # 统一转灰度\n",
        "            img = img.convert(\"L\")\n",
        "        return np.array(img)\n",
        "    raise ValueError(f\"Unsupported file: {path}\")\n",
        "\n",
        "def _zscore(x: np.ndarray, eps=1e-6) -> np.ndarray:\n",
        "    return (x - x.mean()) / (x.std() + eps)\n",
        "\n",
        "def _minmax01(x: np.ndarray, eps=1e-6) -> np.ndarray:\n",
        "    mn, mx = x.min(), x.max()\n",
        "    return (x - mn) / (mx - mn + eps)\n",
        "\n",
        "# --- 新增：把 2D 数据补成 3D（沿指定轴插入一个长度为1的维度） ---\n",
        "def _ensure_3d(arr: np.ndarray, slice_axis: int) -> np.ndarray:\n",
        "    if arr.ndim == 2:\n",
        "        return np.expand_dims(arr, axis=slice_axis)  # 让 shape 在 slice_axis 位置变成 1\n",
        "    return arr\n",
        "\n",
        "class OASIS2DSeg(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        items: List[Dict],               # [{'pid','img','mask'}, ...] —— 来自 split JSON 某一组\n",
        "        slice_axis: int = 2,             # 2=轴状切片（axial）\n",
        "        target_hw: Optional[Tuple[int,int]] = (256, 256),\n",
        "        min_fg_pixels: int = 64,         # 掩码前景像素下限\n",
        "        norm: str = \"zscore\",            # 或 \"minmax\"\n",
        "        augment: bool = False,\n",
        "        rng_seed: int = 42,\n",
        "    ):\n",
        "        self.items = items\n",
        "        self.slice_axis = slice_axis\n",
        "        self.target_hw = target_hw\n",
        "        self.min_fg_pixels = min_fg_pixels\n",
        "        self.norm = norm\n",
        "        self.augment = augment\n",
        "        self.rng = np.random.default_rng(rng_seed)\n",
        "\n",
        "        # 预先构建 (volume_idx, slice_idx) 列表（只保留有前景的切片）\n",
        "        self._slices = []\n",
        "        for vi, it in enumerate(self.items):\n",
        "            img = _load_array(it[\"img\"])\n",
        "            msk = _load_array(it[\"mask\"]).astype(np.int64)\n",
        "            # --- 新增：兼容 2D png，自动补成 3D ---\n",
        "            img = _ensure_3d(img, self.slice_axis)\n",
        "            msk = _ensure_3d(msk, self.slice_axis)\n",
        "            if img.shape != msk.shape:\n",
        "                raise ValueError(f\"Shape mismatch: {img.shape} vs {msk.shape} for {it}\")\n",
        "            num_slices = img.shape[self.slice_axis]\n",
        "            for si in range(num_slices):\n",
        "                sl_msk = np.take(msk, si, axis=self.slice_axis)\n",
        "                if (sl_msk > 0).sum() >= self.min_fg_pixels:\n",
        "                    self._slices.append((vi, si))\n",
        "        if not self._slices:\n",
        "            raise RuntimeError(\"No valid slices; consider lowering min_fg_pixels.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._slices)\n",
        "\n",
        "    def _resize_pair(self, img2d: np.ndarray, msk2d: np.ndarray, target_hw):\n",
        "        if target_hw is None:\n",
        "            return img2d, msk2d\n",
        "        H, W = target_hw\n",
        "        # 图像双线性；掩码最近邻（避免标签污染）\n",
        "        img_r = sk_resize(img2d, (H, W), order=1, preserve_range=True, anti_aliasing=True).astype(np.float32)\n",
        "        msk_r = sk_resize(msk2d, (H, W), order=0, preserve_range=True, anti_aliasing=False).astype(np.int64)\n",
        "        return img_r, msk_r\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vi, si = self._slices[idx]\n",
        "        it = self.items[vi]\n",
        "        img = _load_array(it[\"img\"])\n",
        "        msk = _load_array(it[\"mask\"]).astype(np.int64)\n",
        "        # --- 新增：兼容 2D png，自动补成 3D ---\n",
        "        img = _ensure_3d(img, self.slice_axis)\n",
        "        msk = _ensure_3d(msk, self.slice_axis)\n",
        "\n",
        "        sl_img = np.take(img, si, axis=self.slice_axis).astype(np.float32)\n",
        "        sl_msk = np.take(msk, si, axis=self.slice_axis)\n",
        "\n",
        "        # 归一化\n",
        "        sl_img = _zscore(sl_img) if self.norm == \"zscore\" else _minmax01(sl_img)\n",
        "\n",
        "        # 简单增强（可选）：左右/上下翻转（保持图像与掩码同步）\n",
        "        if self.augment:\n",
        "            if self.rng.random() < 0.5:\n",
        "                sl_img = np.flip(sl_img, axis=1); sl_msk = np.flip(sl_msk, axis=1)\n",
        "            if self.rng.random() < 0.5:\n",
        "                sl_img = np.flip(sl_img, axis=0); sl_msk = np.flip(sl_msk, axis=0)\n",
        "\n",
        "        # 尺寸对齐\n",
        "        sl_img, sl_msk = self._resize_pair(sl_img, sl_msk, self.target_hw)\n",
        "\n",
        "        # ===== 关键修复：把掩码压成类别索引 =====\n",
        "        # 二分类场景：非零即前景 -> {0,1}\n",
        "        sl_msk = (sl_msk > 0).astype(np.int64)\n",
        "\n",
        "        # 如果将来是多分类（例：0,85,170,255 -> {0,1,2,3}），可改为：\n",
        "        # mapping = {0:0, 85:1, 170:2, 255:3}\n",
        "        # sl_msk = np.vectorize(lambda v: mapping.get(int(v), 0), otypes=[np.int64])(sl_msk)\n",
        "\n",
        "        # 转张量\n",
        "        img_t = torch.from_numpy(sl_img[None, ...].astype(np.float32))  # [1,H,W]\n",
        "        msk_t = torch.from_numpy(sl_msk.astype(np.int64))               # [H,W]\n",
        "        meta = {\"pid\": it[\"pid\"], \"slice\": int(si)}\n",
        "        return img_t, msk_t, meta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUgYjOQ3rJiS",
        "outputId": "fe8b584d-77e1-42e5-bb44-6e2d3baf904b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing datasets/oasis2d.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/prepare_split.py\n",
        "# scripts/prepare_split.py\n",
        "# 用法：\n",
        "#   python scripts/prepare_split.py \\\n",
        "#     --data_root /home/groups/comp3710/OASIS_preprocessed \\\n",
        "#     --out data/split_42.json\n",
        "\n",
        "import argparse, json, os, re, random\n",
        "from pathlib import Path\n",
        "\n",
        "# === 扩展：PNG/JPG 支持（原 NIfTI/NumPy 也保留） ===\n",
        "IMG_EXTS = (\".nii\", \".nii.gz\", \".npy\", \".npz\", \".png\", \".jpg\", \".jpeg\")\n",
        "MASK_EXTS = IMG_EXTS\n",
        "\n",
        "def is_img(p: str) -> bool:\n",
        "    p = p.lower()\n",
        "    return any(p.endswith(ext) for ext in IMG_EXTS)\n",
        "\n",
        "def is_mask(p: str) -> bool:\n",
        "    p = p.lower()\n",
        "    return any(p.endswith(ext) for ext in MASK_EXTS)\n",
        "\n",
        "# ---- 关键：把文件名统一为“可配对”的规范形态 ----\n",
        "# 例： \"seg_441_slice_0.nii.png\" -> \"441_slice_0\"\n",
        "#      \"441_slice_0.nii.png\"     -> \"441_slice_0\"\n",
        "#      \"case123_img.png\"         -> \"case123\"\n",
        "#      \"case123_mask.png\"        -> \"case123\"\n",
        "def norm_stem(name_or_path: str) -> str:\n",
        "    name = Path(name_or_path).name\n",
        "    stem = Path(name).stem            # 去掉最后一个扩展名（如 .png）\n",
        "    stem = re.sub(r\"\\.nii$\", \"\", stem, flags=re.I)  # 去掉尾部 .nii\n",
        "    stem = re.sub(r\"^(seg_|mask_|label_|case_)\", \"\", stem, flags=re.I)\n",
        "    stem = re.sub(r\"(_mask|_seg(mentation)?)$\", \"\", stem, flags=re.I)  # 去掉后缀\n",
        "    return stem\n",
        "\n",
        "def find_pairs(data_root: Path):\n",
        "    root = Path(data_root)\n",
        "    pairs = []\n",
        "\n",
        "    # 情况 A：images/ 和 masks/ 兄弟目录（保持，改用 norm_stem 匹配）\n",
        "    img_dir, msk_dir = root / \"images\", root / \"masks\"\n",
        "    if img_dir.is_dir() and msk_dir.is_dir():\n",
        "        imgs = sorted([p for p in img_dir.rglob(\"*\") if p.is_file() and is_img(str(p))])\n",
        "        idx = {norm_stem(p.name): p for p in imgs}\n",
        "        for m in sorted([p for p in msk_dir.rglob(\"*\") if p.is_file() and is_mask(str(p))]):\n",
        "            key = norm_stem(m.name)\n",
        "            cand = idx.get(key)\n",
        "            if cand:\n",
        "                pairs.append({\"pid\": key, \"img\": str(cand), \"mask\": str(m)})\n",
        "        if pairs:\n",
        "            return pairs\n",
        "\n",
        "    # 情况 B：每个病人一个文件夹（保持不变）\n",
        "    for d in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
        "        pid = d.name\n",
        "        imgs = sorted([p for p in d.rglob(\"*\")\n",
        "                       if p.is_file() and is_img(str(p)) and re.search(r\"(?:^|[_-])(img|image|t1|t2)(?:[_-]|$)\", p.stem, re.I)])\n",
        "        msks = sorted([p for p in d.rglob(\"*\")\n",
        "                       if p.is_file() and is_mask(str(p)) and re.search(r\"(?:^|[_-])(mask|label|seg)(?:[_-]|$)\", p.stem, re.I)])\n",
        "        if imgs and msks:\n",
        "            pairs.append({\"pid\": pid, \"img\": str(imgs[0]), \"mask\": str(msks[0])})\n",
        "    if pairs:\n",
        "        return pairs\n",
        "\n",
        "    # 情况 C：扁平目录，<PID>_img.* / <PID>_mask.*（保持，改用 norm_stem）\n",
        "    files = sorted([p for p in root.rglob(\"*\") if p.is_file() and (is_img(str(p)) or is_mask(str(p)))])\n",
        "    if files:\n",
        "        imgs = [p for p in files if re.search(r\"(?:^|[_-])(img|image|t1|t2)(?:[_-]|$)\", p.stem, re.I)]\n",
        "        msks = [p for p in files if re.search(r\"(?:^|[_-])(mask|label|seg)(?:[_-]|$)\", p.stem, re.I)]\n",
        "        idx = {norm_stem(p.name): p for p in imgs}\n",
        "        tmp_pairs = []\n",
        "        for m in msks:\n",
        "            key = norm_stem(m.name)\n",
        "            if key in idx:\n",
        "                tmp_pairs.append({\"pid\": key, \"img\": str(idx[key]), \"mask\": str(m)})\n",
        "        if tmp_pairs:\n",
        "            return tmp_pairs\n",
        "\n",
        "    # 情况 D：Keras PNG 切片版目录（新增；核心匹配规则改为 norm_stem）\n",
        "    #   keras_png_slices_train/            keras_png_slices_seg_train/\n",
        "    #   keras_png_slices_validate/         keras_png_slices_seg_validate/\n",
        "    #   keras_png_slices_test/             keras_png_slices_seg_test/\n",
        "    png_pairs = []\n",
        "    phases = [\"train\", \"validate\", \"val\", \"test\"]  # validate/val 都兼容\n",
        "    for ph in phases:\n",
        "        imgd = root / f\"keras_png_slices_{ph}\"\n",
        "        mskd = root / f\"keras_png_slices_seg_{ph}\"\n",
        "        if not (imgd.is_dir() and mskd.is_dir()):\n",
        "            continue\n",
        "\n",
        "        # 掩码映射：用“规范 stem”做键\n",
        "        mask_idx = {}\n",
        "        for m in mskd.rglob(\"*\"):\n",
        "            if m.is_file() and m.suffix.lower() in (\".png\", \".jpg\", \".jpeg\"):\n",
        "                mask_idx[norm_stem(m.name)] = m\n",
        "\n",
        "        # 找图片并配对\n",
        "        for im in imgd.rglob(\"*\"):\n",
        "            if not (im.is_file() and im.suffix.lower() in (\".png\", \".jpg\", \".jpeg\")):\n",
        "                continue\n",
        "            key = norm_stem(im.name)\n",
        "            cand = mask_idx.get(key)\n",
        "            if cand is not None:\n",
        "                # 每个切片当作独立“样本”；pid 用 key（稳妥）\n",
        "                png_pairs.append({\"pid\": key, \"img\": str(im), \"mask\": str(cand)})\n",
        "\n",
        "    if png_pairs:\n",
        "        return sorted(png_pairs, key=lambda x: x[\"pid\"])\n",
        "\n",
        "    # 实在找不到\n",
        "    return []\n",
        "\n",
        "def split_by_patient(pairs, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    # 聚合 pid → 该 pid 下的全部条目（PNG 场景里通常 1 条/切片）\n",
        "    by_pid = {}\n",
        "    for it in pairs:\n",
        "        by_pid.setdefault(it[\"pid\"], []).append(it)\n",
        "    pids = sorted(by_pid.keys())\n",
        "    rng = random.Random(seed)\n",
        "    rng.shuffle(pids)\n",
        "\n",
        "    n = len(pids)\n",
        "    n_test = int(n * test_ratio)\n",
        "    n_val  = int(n * val_ratio)\n",
        "    test_p = set(pids[:n_test])\n",
        "    val_p  = set(pids[n_test:n_test + n_val])\n",
        "\n",
        "    split = {\"train\": [], \"val\": [], \"test\": []}\n",
        "    for pid in pids:\n",
        "        bucket = \"test\" if pid in test_p else (\"val\" if pid in val_p else \"train\")\n",
        "        split[bucket].extend(by_pid[pid])\n",
        "    return split\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data_root\", required=True)\n",
        "    ap.add_argument(\"--out\", default=\"data/split_42.json\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--val_ratio\", type=float, default=0.15)\n",
        "    ap.add_argument(\"--test_ratio\", type=float, default=0.15)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    pairs = find_pairs(Path(args.data_root))\n",
        "    if not pairs:\n",
        "        raise SystemExit(f\"[ERR] No image/mask pairs found under {args.data_root}\")\n",
        "\n",
        "    split = split_by_patient(pairs, args.val_ratio, args.test_ratio, args.seed)\n",
        "\n",
        "    out_path = Path(args.out)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(split, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"[OK] volumes: train={len(split['train'])}, val={len(split['val'])}, test={len(split['test'])}\")\n",
        "    print(f\"[OK] saved to {str(out_path)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkSCK-cWvLrN",
        "outputId": "2018baff-b156-4be9-d689-5a21f5dc5279"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/prepare_split.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/train_unet.py\n",
        "# scripts/train_unet.py\n",
        "import os, json, math, argparse, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets.oasis2d import OASIS2DSeg\n",
        "\n",
        "# -------------------------\n",
        "# 1) UNet 定义（简洁稳定版）\n",
        "# -------------------------\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "    def forward(self, x): return self.conv(self.pool(x))\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
        "            self.conv = DoubleConv(in_ch, out_ch)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_ch, out_ch)\n",
        "        self.bilinear = bilinear\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # pad/crop to match skip\n",
        "        diffY = x2.size(2) - x1.size(2)\n",
        "        diffX = x2.size(3) - x1.size(3)\n",
        "        x1 = F.pad(x1, [diffX//2, diffX - diffX//2, diffY//2, diffY - diffY//2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=1, n_classes=4, base=32, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.inc  = DoubleConv(in_ch, base)\n",
        "        self.down1= Down(base, base*2)\n",
        "        self.down2= Down(base*2, base*4)\n",
        "        self.down3= Down(base*4, base*8)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4= Down(base*8, base*16//factor)\n",
        "        self.up1  = Up(base*16, base*8//factor, bilinear)\n",
        "        self.up2  = Up(base*8,  base*4//factor, bilinear)\n",
        "        self.up3  = Up(base*4,  base*2//factor, bilinear)\n",
        "        self.up4  = Up(base*2,  base, bilinear)\n",
        "        self.outc = nn.Conv2d(base, n_classes, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x,  x3)\n",
        "        x = self.up3(x,  x2)\n",
        "        x = self.up4(x,  x1)\n",
        "        return self.outc(x)  # logits [B,C,H,W]\n",
        "\n",
        "# -------------------------\n",
        "# 2) 损失 & 评估\n",
        "# -------------------------\n",
        "def soft_dice_loss(logits, target, smooth=1e-5, ignore_background=True):\n",
        "    \"\"\"\n",
        "    logits: [B,C,H,W], target: [B,H,W] long\n",
        "    返回 (1 - mean foreground dice)\n",
        "    \"\"\"\n",
        "    num_classes = logits.shape[1]\n",
        "    probs = F.softmax(logits, dim=1)                         # [B,C,H,W]\n",
        "    one_hot = F.one_hot(target, num_classes).permute(0,3,1,2).float()\n",
        "    dims = (0,2,3)\n",
        "    inter = (probs * one_hot).sum(dims)                      # [C]\n",
        "    denom = probs.sum(dims) + one_hot.sum(dims)              # [C]\n",
        "    dice = (2*inter + smooth) / (denom + smooth)             # [C]\n",
        "    if ignore_background and num_classes > 1:\n",
        "        dice = dice[1:]\n",
        "    return 1.0 - dice.mean()\n",
        "\n",
        "@torch.no_grad()\n",
        "def dsc_per_class(logits, target, num_classes, ignore_background=False):\n",
        "    pred = logits.argmax(1)  # [B,H,W]\n",
        "    dsc = []\n",
        "    classes = range(num_classes)\n",
        "    if ignore_background and num_classes > 1:\n",
        "        classes = range(1, num_classes)\n",
        "    for c in classes:\n",
        "        p = (pred == c)\n",
        "        t = (target == c)\n",
        "        tp = (p & t).sum().item()\n",
        "        denom = p.sum().item() + t.sum().item()\n",
        "        dsc_c = (2*tp) / (denom + 1e-5) if denom > 0 else 1.0\n",
        "        dsc.append(dsc_c)\n",
        "    return dsc  # 前景类列表\n",
        "\n",
        "# -------------------------\n",
        "# 3) 训练循环\n",
        "# -------------------------\n",
        "def train_one_epoch(model, loader, opt, scaler, device, num_classes, lambda_dice=1.0):\n",
        "    model.train()\n",
        "    run_loss, n = 0.0, 0\n",
        "    for img, msk, _ in loader:\n",
        "        img, msk = img.to(device, non_blocking=True), msk.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type=='cuda')):\n",
        "            logits = model(img)\n",
        "            ce = F.cross_entropy(logits, msk)                     # CE 不需要 one-hot\n",
        "            dice = soft_dice_loss(logits, msk, ignore_background=True)\n",
        "            loss = ce + lambda_dice * dice\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        run_loss += loss.item() * img.size(0)\n",
        "        n += img.size(0)\n",
        "    return run_loss / max(1,n)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, device, num_classes):\n",
        "    model.eval()\n",
        "    dsc_sum = np.zeros(max(1, num_classes-1), dtype=np.float64)  # 默认不计背景\n",
        "    n_batches = 0\n",
        "    for img, msk, _ in loader:\n",
        "        img, msk = img.to(device), msk.to(device)\n",
        "        logits = model(img)\n",
        "        dsc = dsc_per_class(logits, msk, num_classes, ignore_background=True)\n",
        "        dsc_sum += np.array(dsc, dtype=np.float64)\n",
        "        n_batches += 1\n",
        "    mean_per_class = (dsc_sum / max(1, n_batches)).tolist()\n",
        "    macro = float(np.mean(mean_per_class)) if len(mean_per_class) else 0.0\n",
        "    return macro, mean_per_class\n",
        "\n",
        "# -------------------------\n",
        "# 4) 主函数 & 参数\n",
        "# -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data_root\", required=False, help=\"仅用于信息记录，Dataset 使用 split JSON 内路径\")\n",
        "    ap.add_argument(\"--split_json\", required=True, help=\"Step2 生成的 JSON\")\n",
        "    ap.add_argument(\"--save_dir\", default=\"results/exp1\")\n",
        "    ap.add_argument(\"--in_channels\", type=int, default=1)\n",
        "    ap.add_argument(\"--num_classes\", type=int, default=4, help=\"分割类别数（含背景）\")\n",
        "    ap.add_argument(\"--base\", type=int, default=32)\n",
        "    ap.add_argument(\"--epochs\", type=int, default=100)\n",
        "    ap.add_argument(\"--batch_size\", type=int, default=16)\n",
        "    ap.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    ap.add_argument(\"--weight_decay\", type=float, default=1e-5)\n",
        "    ap.add_argument(\"--lambda_dice\", type=float, default=1.0)\n",
        "    ap.add_argument(\"--amp\", action=\"store_true\")\n",
        "    ap.add_argument(\"--num_workers\", type=int, default=4)\n",
        "    ap.add_argument(\"--target_h\", type=int, default=256)\n",
        "    ap.add_argument(\"--target_w\", type=int, default=256)\n",
        "    ap.add_argument(\"--min_fg_pixels\", type=int, default=64)\n",
        "    ap.add_argument(\"--patience\", type=int, default=15)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # 读取 split\n",
        "    split = json.load(open(args.split_json, \"r\"))\n",
        "    tr_items = split[\"train\"]\n",
        "    va_items = split[\"val\"]\n",
        "\n",
        "    train_ds = OASIS2DSeg(\n",
        "        tr_items, target_hw=(args.target_h, args.target_w),\n",
        "        augment=True,  min_fg_pixels=args.min_fg_pixels\n",
        "    )\n",
        "    val_ds = OASIS2DSeg(\n",
        "        va_items, target_hw=(args.target_h, args.target_w),\n",
        "        augment=False, min_fg_pixels=args.min_fg_pixels\n",
        "    )\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,\n",
        "                              num_workers=args.num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=args.batch_size, shuffle=False,\n",
        "                              num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "    # 模型 & 优化\n",
        "    model = UNet(in_ch=args.in_channels, n_classes=args.num_classes, base=args.base).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", patience=5, factor=0.5)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(args.amp and device.type==\"cuda\"))\n",
        "\n",
        "    best_macro = -1.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        t0 = time.time()\n",
        "        tr_loss = train_one_epoch(model, train_loader, opt, scaler, device,\n",
        "                                  num_classes=args.num_classes, lambda_dice=args.lambda_dice)\n",
        "        val_macro, val_per_class = validate(model, val_loader, device, args.num_classes)\n",
        "        scheduler.step(val_macro)\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        per_class_str = \", \".join([f\"{d:.4f}\" for d in val_per_class])\n",
        "        print(f\"[Epoch {epoch:03d}] loss={tr_loss:.4f}  val_macro_DSC={val_macro:.4f}  \"\n",
        "              f\"per-class (fg)=[{per_class_str}]  time={dt:.1f}s\")\n",
        "\n",
        "        # 早停 & 保存最优\n",
        "        if val_macro > best_macro:\n",
        "            best_macro = val_macro\n",
        "            epochs_no_improve = 0\n",
        "            ckpt_path = os.path.join(args.save_dir, \"best.ckpt\")\n",
        "            torch.save({\"model\": model.state_dict(),\n",
        "                        \"num_classes\": args.num_classes,\n",
        "                        \"in_channels\": args.in_channels}, ckpt_path)\n",
        "            print(f\"  ↳ Saved best checkpoint to {ckpt_path}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= args.patience:\n",
        "                print(f\"Early stopping at epoch {epoch}. Best macro DSC={best_macro:.4f}\")\n",
        "                break\n",
        "\n",
        "    print(\"Training finished. Best macro DSC (val) =\", best_macro)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fCSytbDvO3F",
        "outputId": "4a93c387-8797-4732-b2a0-b63efa594f40"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/train_unet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/prepare_split.py \\\n",
        "  --data_root \"/content/data/keras_png_slices_data\" \\\n",
        "  --out \"data/split_42.json\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1yo7uRzvweJ",
        "outputId": "fd4d6494-15b5-490c-9b6f-44aa4dd3579f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] volumes: train=7930, val=1699, test=1699\n",
            "[OK] saved to data/split_42.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!python -m scripts.train_unet \\\n",
        "  --split_json data/split_42.json \\\n",
        "  --save_dir results/exp1 \\\n",
        "  --epochs 30 \\\n",
        "  --batch_size 16 \\\n",
        "  --lr 1e-3 \\\n",
        "  --amp \\\n",
        "  --num_workers 2 \\\n",
        "  --num_classes 2 \\\n",
        "  --target_h 256 --target_w 256 \\\n",
        "  --min_fg_pixels 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UyrUaWlwBkh",
        "outputId": "6ee7cd0a-ccad-4c11-bc90-aff07eab8500"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Device: cuda\n",
            "/content/scripts/train_unet.py:199: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(args.amp and device.type==\"cuda\"))\n",
            "[Epoch 001] loss=0.0837  val_macro_DSC=0.9864  per-class (fg)=[0.9864]  time=27.7s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 002] loss=0.0307  val_macro_DSC=0.9881  per-class (fg)=[0.9881]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 003] loss=0.0231  val_macro_DSC=0.9886  per-class (fg)=[0.9886]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 004] loss=0.0191  val_macro_DSC=0.9927  per-class (fg)=[0.9927]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 005] loss=0.0167  val_macro_DSC=0.9928  per-class (fg)=[0.9928]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 006] loss=0.0149  val_macro_DSC=0.9944  per-class (fg)=[0.9944]  time=24.2s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 007] loss=0.0139  val_macro_DSC=0.9941  per-class (fg)=[0.9941]  time=24.0s\n",
            "[Epoch 008] loss=0.0127  val_macro_DSC=0.9957  per-class (fg)=[0.9957]  time=23.9s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 009] loss=0.0107  val_macro_DSC=0.9954  per-class (fg)=[0.9954]  time=23.9s\n",
            "[Epoch 010] loss=0.0100  val_macro_DSC=0.9963  per-class (fg)=[0.9963]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 011] loss=0.0093  val_macro_DSC=0.9957  per-class (fg)=[0.9957]  time=24.1s\n",
            "[Epoch 012] loss=0.0089  val_macro_DSC=0.9966  per-class (fg)=[0.9966]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 013] loss=0.0098  val_macro_DSC=0.9964  per-class (fg)=[0.9964]  time=24.0s\n",
            "[Epoch 014] loss=0.0084  val_macro_DSC=0.9964  per-class (fg)=[0.9964]  time=24.0s\n",
            "[Epoch 015] loss=0.0077  val_macro_DSC=0.9968  per-class (fg)=[0.9968]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 016] loss=0.0072  val_macro_DSC=0.9971  per-class (fg)=[0.9971]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 017] loss=0.0069  val_macro_DSC=0.9966  per-class (fg)=[0.9966]  time=24.0s\n",
            "[Epoch 018] loss=0.0064  val_macro_DSC=0.9972  per-class (fg)=[0.9972]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 019] loss=0.0063  val_macro_DSC=0.9978  per-class (fg)=[0.9978]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 020] loss=0.0058  val_macro_DSC=0.9980  per-class (fg)=[0.9980]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 021] loss=0.0054  val_macro_DSC=0.9980  per-class (fg)=[0.9980]  time=24.1s\n",
            "[Epoch 022] loss=0.0052  val_macro_DSC=0.9981  per-class (fg)=[0.9981]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 023] loss=0.0049  val_macro_DSC=0.9980  per-class (fg)=[0.9980]  time=24.1s\n",
            "[Epoch 024] loss=0.0049  val_macro_DSC=0.9965  per-class (fg)=[0.9965]  time=24.0s\n",
            "[Epoch 025] loss=0.0046  val_macro_DSC=0.9981  per-class (fg)=[0.9981]  time=23.9s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 026] loss=0.0043  val_macro_DSC=0.9982  per-class (fg)=[0.9982]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 027] loss=0.0042  val_macro_DSC=0.9982  per-class (fg)=[0.9982]  time=24.0s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 028] loss=0.0041  val_macro_DSC=0.9981  per-class (fg)=[0.9981]  time=23.9s\n",
            "[Epoch 029] loss=0.0039  val_macro_DSC=0.9984  per-class (fg)=[0.9984]  time=23.9s\n",
            "  ↳ Saved best checkpoint to results/exp1/best.ckpt\n",
            "[Epoch 030] loss=0.0039  val_macro_DSC=0.9983  per-class (fg)=[0.9983]  time=24.0s\n",
            "Training finished. Best macro DSC (val) = 0.9984431338200386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/infer_unet.py\n",
        "import os, json, argparse\n",
        "import numpy as np\n",
        "import torch, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets.oasis2d import OASIS2DSeg\n",
        "from scripts.train_unet import UNet, dsc_per_class\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--split_json\", required=True)\n",
        "    ap.add_argument(\"--subset\", default=\"test\", choices=[\"train\",\"val\",\"test\"])\n",
        "    ap.add_argument(\"--ckpt\", required=True)\n",
        "    ap.add_argument(\"--out_dir\", default=\"results/vis\")\n",
        "    ap.add_argument(\"--batch_size\", type=int, default=16)\n",
        "    ap.add_argument(\"--num_workers\", type=int, default=2)\n",
        "    ap.add_argument(\"--target_h\", type=int, default=256)\n",
        "    ap.add_argument(\"--target_w\", type=int, default=256)\n",
        "    ap.add_argument(\"--num_images\", type=int, default=24)    # 保存前多少张三联图\n",
        "    ap.add_argument(\"--ignore_bg\", action=\"store_true\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    # 读 split\n",
        "    split = json.load(open(args.split_json))\n",
        "    items = split[args.subset]\n",
        "\n",
        "    # 加载 ckpt\n",
        "    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n",
        "    num_classes = int(ckpt.get(\"num_classes\", 2))\n",
        "    in_ch       = int(ckpt.get(\"in_channels\", 1))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = UNet(in_ch=in_ch, n_classes=num_classes, base=32).to(device)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    model.eval()\n",
        "\n",
        "    # Dataset/DataLoader\n",
        "    ds = OASIS2DSeg(items, target_hw=(args.target_h, args.target_w), augment=False, min_fg_pixels=0)\n",
        "    dl = DataLoader(ds, batch_size=args.batch_size, shuffle=False,\n",
        "                    num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "    # 评测\n",
        "    dsc_sum = np.zeros(max(1, num_classes-1), dtype=np.float64)\n",
        "    n_batches = 0\n",
        "\n",
        "    saved = 0\n",
        "    with torch.no_grad():\n",
        "        for (img, msk, meta) in dl:\n",
        "            img = img.to(device)\n",
        "            msk = msk.to(device)\n",
        "            logits = model(img)\n",
        "\n",
        "            # 统计 DSC（前景类；可选忽略背景）\n",
        "            dsc_list = dsc_per_class(logits, msk, num_classes, ignore_background=args.ignore_bg or (num_classes>1))\n",
        "            dsc_sum += np.array(dsc_list, dtype=np.float64)\n",
        "            n_batches += 1\n",
        "\n",
        "            # 前 num_images 张保存三联图\n",
        "            for b in range(img.size(0)):\n",
        "                if saved >= args.num_images: break\n",
        "                pred = logits[b:b+1].argmax(1).squeeze(0).detach().cpu().numpy()\n",
        "                im   = img[b,0].detach().cpu().numpy()    # [H,W]\n",
        "                gt   = msk[b].detach().cpu().numpy()\n",
        "\n",
        "                # 保存\n",
        "                fig = plt.figure(figsize=(9,3))\n",
        "                ax1 = fig.add_subplot(1,3,1); ax1.set_title(\"Image\"); ax1.imshow(im, cmap=\"gray\"); ax1.axis(\"off\")\n",
        "                ax2 = fig.add_subplot(1,3,2); ax2.set_title(\"Mask\");  ax2.imshow(gt, vmin=0, vmax=num_classes-1); ax2.axis(\"off\")\n",
        "                ax3 = fig.add_subplot(1,3,3); ax3.set_title(\"Pred\");  ax3.imshow(pred, vmin=0, vmax=num_classes-1); ax3.axis(\"off\")\n",
        "                pid = meta[\"pid\"][b] if isinstance(meta[\"pid\"], list) else meta[\"pid\"]\n",
        "                out_path = os.path.join(args.out_dir, f\"{saved:03d}_{pid}.png\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(out_path, dpi=120)\n",
        "                plt.close(fig)\n",
        "                saved += 1\n",
        "            if saved >= args.num_images: break\n",
        "\n",
        "    mean_per_class = (dsc_sum / max(1, n_batches)).tolist()\n",
        "    macro = float(np.mean(mean_per_class)) if len(mean_per_class) else 0.0\n",
        "\n",
        "    print(f\"[{args.subset}] macro DSC = {macro:.4f}\")\n",
        "    print(f\"[{args.subset}] per-class (fg) = {[round(x,4) for x in mean_per_class]}\")\n",
        "    print(f\"Saved {saved} preview images under: {args.out_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL0bNcyeRDxZ",
        "outputId": "8e3fc7a6-50e8-496c-8526-16e15b7f06cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/infer_unet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!python -m scripts.infer_unet \\\n",
        "  --split_json data/split_42.json \\\n",
        "  --subset test \\\n",
        "  --ckpt results/exp1/best.ckpt \\\n",
        "  --out_dir results/exp1/vis_test \\\n",
        "  --num_images 24 \\\n",
        "  --ignore_bg \\\n",
        "  --target_h 256 --target_w 256\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPadJXapRG0U",
        "outputId": "f39d467c-fe72-4273-d5cd-6b8d1fbc701c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "[test] macro DSC = 0.9984\n",
            "[test] per-class (fg) = [0.9984]\n",
            "Saved 24 preview images under: results/exp1/vis_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, json, os, numpy as np\n",
        "from datasets.oasis2d import OASIS2DSeg\n",
        "from scripts.train_unet import UNet\n",
        "import torch.nn.functional as F\n",
        "\n",
        "ckpt = torch.load(\"results/exp1/best.ckpt\", map_location=\"cpu\")\n",
        "num_classes = int(ckpt.get(\"num_classes\", 2))\n",
        "model = UNet(in_ch=int(ckpt.get(\"in_channels\",1)), n_classes=num_classes, base=32).eval().cuda()\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "\n",
        "split = json.load(open(\"data/split_42.json\"))\n",
        "ds = OASIS2DSeg(split[\"test\"][:1], target_hw=(256,256), augment=False)\n",
        "img, msk, meta = ds[0]\n",
        "with torch.no_grad():\n",
        "    logits = model(img.unsqueeze(0).cuda())      # [1,C,H,W]\n",
        "    probs  = F.softmax(logits, dim=1)            # categorical 概率\n",
        "    pred   = probs.argmax(1)                     # [1,H,W] 类索引\n",
        "    onehot = F.one_hot(pred, num_classes).permute(0,3,1,2).byte()  # [1,C,H,W] one-hot\n",
        "\n",
        "os.makedirs(\"results/exp1/onehot_demo\", exist_ok=True)\n",
        "np.save(\"results/exp1/onehot_demo/pred_onehot.npy\", onehot[0].cpu().numpy())  # 保存 one-hot\n",
        "np.save(\"results/exp1/onehot_demo/pred_index.npy\",  pred[0].cpu().numpy())    # 保存类索引\n",
        "print(\"Saved:\", \"results/exp1/onehot_demo/pred_onehot.npy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2HiQr6LTOEM",
        "outputId": "ad327597-335e-4dff-f845-ae327b654c8b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: results/exp1/onehot_demo/pred_onehot.npy\n"
          ]
        }
      ]
    }
  ]
}